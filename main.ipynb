{
 "cells": [  
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addressing Student Mental Health and Well-being: A Chatbot Solution\n",
    "\n",
    "In response to the growing concern around student mental health, I developed a simple chatbot using Python's NLTK library. This chatbot aims to provide accessible information and resources on mental health issues commonly faced by students in college, as well as offer solutions and practices to promote well-being.\n",
    "\n",
    "### Problem Statement\n",
    "**PS 2:** Student mental health is a critical issue. The objective is to create a solution that integrates mental health resources, promotes mindfulness practices, or creates a safe space for students to seek support.\n",
    "\n",
    "### Overview\n",
    "The chatbot leverages natural language processing (NLP) to interact with users, offering guidance and information based on a customized corpus. This corpus consists of documents that address various aspects of student mental health, including common challenges and effective strategies for managing stress and anxiety. The chatbot serves as a supportive tool, providing responses tailored to the user's input.  [ The corpus is 'corpus.txt' ]\n",
    "\n",
    "\n",
    "### A Demo of the Chatbot in action  [ Open Chatbot in cmd for better experience ]\n",
    "<img src=\"https://raw.githubusercontent.com/souvikcseiitk/my-CV/main/extras/animation.gif\" alt=\"Animation\" width=\"600\" height=\"338\" />\n",
    "\n",
    "\n",
    "\n",
    "### Features\n",
    "- **Keyword Matching:** The bot uses keyword matching to recognize greetings and respond appropriately.\n",
    "- **TF-IDF Vectorization:** Text is transformed into a numerical format, enabling the bot to assess and compare the relevance of responses.\n",
    "- **Cosine Similarity:** The bot evaluates the similarity between user queries and potential responses to generate the most suitable answer.\n",
    "- **Lemmatization:** Text is normalized, ensuring that the bot recognizes different forms of a word as equivalent.\n",
    "\n",
    "### Usage\n",
    "To interact with the chatbot, simply run the program in your terminal. It will greet you and begin responding to your queries. The chatbot is designed to help students find useful information and support related to mental health issues. To exit, type \"Bye.\"\n",
    "\n",
    "python chatbot.py\n",
    "\n",
    "### Conclusion\n",
    "This chatbot is a small but meaningful step towards addressing student mental health by making information and resources more accessible. By integrating NLP techniques, the bot creates a supportive environment where students can seek guidance on mental well-being."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing NLTK(Natural Language Toolkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('popular', quiet=True) # for downloading packages\n",
    "#nltk.download('punkt') # first-time use only\n",
    "#nltk.download('wordnet') # first-time use only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import random\n",
    "import string # to process standard python strings\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('corpus.txt','r',errors = 'ignore')\n",
    "raw=f.read()\n",
    "raw = raw.lower()# converts to lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
    "word_tokens = nltk.word_tokenize(raw)# converts to list of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keyword matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "def greeting(sentence):\n",
    " \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBO: My name is Robo. I will answer your queries about Mental Health. If you want to exit, type Bye!\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"ROBO: My name is Robo. I will answer your queries about Mental Health. If you want to exit, type Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"ROBO: You are welcome..\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"ROBO: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"ROBO: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"ROBO: Bye! take care..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data for corpus Citation:     \n",
    "[1] https://www.ucas.com/file/513961/download?token=wAaKRniC      \n",
    "[2] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4527955/    \n",
    "\n",
    "# Requirements.txt\n",
    "\n",
    "numpy   \n",
    "scikit-learn   \n",
    "nltk    \n",
    "io   \n",
    "random   \n",
    "string   \n",
    "warnings   \n",
    "\n",
    "\n",
    "# Aknowledgement\n",
    "\n",
    "* Thank you to everyone mentioned below :)\n",
    "* This project development would not have been possible without your support:  \n",
    "\n",
    "\n",
    "\n",
    "Hackconclave-24 IIT Guwahati\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/souvikcseiitk/my-CV/main/extras/IITG.png\" alt=\"Hackconclave24 IITG\" width=\"200\" height=\"150\" />  \n",
    "\n",
    "Open-AI, Chat-GPT\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/souvikcseiitk/my-CV/main/extras/GPT.PNG\" alt=\"GPT\" width=\"450\" height=\"150\" /> \n",
    "\n",
    "Stack Overflow\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/souvikcseiitk/my-CV/main/extras/SO.PNG\" alt=\"SO\" width=\"500\" height=\"120\" /> \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
